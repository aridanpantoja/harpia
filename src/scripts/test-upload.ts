/** biome-ignore-all lint/suspicious/noConsole: this is a script */
/** biome-ignore-all lint/nursery/noAwaitInLoop: this is a script */

import { openai } from '@ai-sdk/openai';
import { createClient } from '@supabase/supabase-js';
import { embedMany, generateObject } from 'ai';
import 'dotenv/config';
import fs from 'node:fs/promises';
import path from 'node:path';

import { PDFLoader } from '@langchain/community/document_loaders/fs/pdf';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { drizzle } from 'drizzle-orm/postgres-js';
import pdf from 'pdf-parse';
import postgres from 'postgres';
import { z } from 'zod';
import { file, fileChunk } from '@/lib/db/schema';

(async () => {
  try {
    console.log('ğŸš€ Iniciando o script...');

    const supabaseUrl = process.env.SUPABASE_URL;
    const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY;

    if (!supabaseUrl) {
      throw new Error('VariÃ¡vel de ambiente SUPABASE_URL nÃ£o definida.');
    }

    if (!supabaseKey) {
      throw new Error(
        'VariÃ¡vel de ambiente SUPABASE_SERVICE_ROLE_KEY nÃ£o definida.'
      );
    }

    const connection = postgres(process.env.DATABASE_URL as string);
    const db = drizzle(connection);

    const supabase = createClient(supabaseUrl, supabaseKey);
    const embeddingModel = openai.textEmbeddingModel('text-embedding-3-small');
    const chatModel = openai('gpt-5-nano');

    console.log('ğŸ“„ Lendo o arquivo PDF...');

    const filePath =
      './src/scripts/uploads/Edital_05_2024_COPERPS_PS2025_retificado_v3.pdf';
    const filename = path.basename(filePath);
    const fileBody = await fs.readFile(filePath);

    const { data: storageData, error: storageError } = await supabase.storage
      .from('harpia')
      .upload(`uploads/${filename}`, fileBody, {
        contentType: 'application/pdf',
        upsert: false,
      });

    if (storageError) {
      throw storageError;
    }

    console.log('âœ… Upload concluÃ­do!');

    console.log('ğŸ” Extraindo metadados e carregando pÃ¡ginas com PDFLoader...');

    const loader = new PDFLoader(filePath);
    const pageLevelDocs = await loader.load();
    const numPages = pageLevelDocs.length;

    const parsedPdfData = await pdf(await fs.readFile(filePath));
    const fileLevelMetadata = parsedPdfData.info;

    console.log('ğŸ§  Gerando categoria e descriÃ§Ã£o com IA...');

    const fullTextForAI = pageLevelDocs
      .map((doc) => doc.pageContent)
      .join('\n');

    const cleanFullTextForAI = fullTextForAI
      .replace(/\s+/g, ' ')
      .replace(/\n\s*\n/g, '\n')
      .trim();

    const { object: fileAiMetadata } = await generateObject({
      model: chatModel,
      schema: z
        .object({})
        .catchall(z.any())
        .describe('InformaÃ§Ãµes especÃ­ficas extraÃ­das do documento'),
      prompt: `
        VocÃª Ã© um assistente especialista em analisar documentos de processos seletivos e editais.

        Receba as informaÃ§Ãµes abaixo sobre o documento, leia atentamente o conteÃºdo e gere um objeto JSON com **todas as informaÃ§Ãµes importantes que possam ajudar alguÃ©m a consultar e entender o documento rapidamente.**

        NÃ£o siga um formato rÃ­gido, mas crie propriedades claras e Ãºteis. A estrutura do objeto deve refletir o mÃ¡ximo de detalhes que conseguir extrair, incluindo mas nÃ£o limitado a:

        - Categoria do documento (ex: Edital, Edital Retificado, Resultado, HomologaÃ§Ã£o, Nota de aviso, etc.)
        - Uma descriÃ§Ã£o concisa e informativa do documento
        - Se o documento for um edital, extraia:
          - NÃºmero total de vagas
          - Vagas distribuÃ­das por campus ou regiÃ£o
          - Cursos disponÃ­veis (tanto por campus/regiÃ£o quanto sem distinÃ§Ã£o de campus)
          - DocumentaÃ§Ã£o necessÃ¡ria para cada tipo de cota, incluindo as cotas mais complexas (ex: cota racial, social, indÃ­gena, deficiente, etc.)
          - Datas importantes, especialmente o perÃ­odo de inscriÃ§Ã£o (data inÃ­cio e data fim)
          - Detalhes dos anexos: quantidade, pÃ¡ginas de inÃ­cio e fim, descriÃ§Ã£o dos anexos se possÃ­vel
          - Estrutura detalhada do documento: capÃ­tulos e subcapÃ­tulos, com tÃ­tulo e pÃ¡ginas (inÃ­cio e fim)
          - NÃºmero e descriÃ§Ã£o de tabelas, quadros, imagens ou grÃ¡ficos presentes
        - Qualquer outra informaÃ§Ã£o relevante para consultas rÃ¡pidas, como regras especÃ­ficas, instruÃ§Ãµes importantes, observaÃ§Ãµes, exceÃ§Ãµes e procedimentos especiais

        Formate a resposta como um objeto JSON que contenha todas as propriedades que considerar relevantes, sempre priorizando a completude e clareza das informaÃ§Ãµes. 

        Se alguma informaÃ§Ã£o nÃ£o estiver presente no documento, simplesmente nÃ£o inclua a propriedade no JSON.

        Aqui estÃ£o os dados do documento:

        Nome do arquivo: "${filename}"
        NÃºmero total de pÃ¡ginas: ${numPages}
        ConteÃºdo completo: "${cleanFullTextForAI}"

        Retorne somente o objeto JSON.
      `,
    });

    console.log('ğŸ’¾ Salvando metadados do arquivo principal...');

    const [fileRecord] = await db
      .insert(file)
      .values({
        filename,
        url: storageData.path,
        category: fileAiMetadata.category || 'Documento',
        description: fileAiMetadata.description || 'Documento processado',
        pages: numPages,
        metadata: {
          file: {
            ...fileLevelMetadata,
          },
          ...fileAiMetadata,
        },
      })
      .returning();

    console.log(`âœ… Registro do arquivo salvo com ID: ${fileRecord.id}`);

    console.log(
      'ğŸ”ª Dividindo o texto e calculando posiÃ§Ãµes (caractere e linha)...'
    );

    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 0,
    });

    const documentsToProcess: {
      content: string;
      metadata: {
        pageNumber: number;
        loc: {
          characters: {
            from: number;
            to: number;
          };
          lines: {
            from: number;
            to: number;
          };
        };
      };
    }[] = [];

    for (const doc of pageLevelDocs) {
      const pageText = doc.pageContent;
      const pageNumber = doc.metadata.loc.pageNumber;
      const chunks = await splitter.splitText(pageText);

      let searchStartIndex = 0;

      for (const chunkContent of chunks) {
        const startIndex = pageText.indexOf(chunkContent, searchStartIndex);

        if (startIndex === -1) {
          continue;
        }

        const endIndex = startIndex + chunkContent.length;
        const startLine = pageText.substring(0, startIndex).split('\n').length;
        const endLine = pageText.substring(0, endIndex).split('\n').length;

        // Limpar o conteÃºdo removendo espaÃ§os extras e normalizando quebras de linha
        const cleanContent = chunkContent
          .replace(/\s+/g, ' ')
          .replace(/\n\s*\n/g, '\n')
          .trim();

        documentsToProcess.push({
          content: cleanContent,
          metadata: {
            pageNumber,
            loc: {
              characters: {
                from: startIndex,
                to: endIndex,
              },
              lines: {
                from: startLine,
                to: endLine,
              },
            },
          },
        });

        searchStartIndex = startIndex + 1;
      }
    }

    console.log(
      `âœ… Texto dividido em ${documentsToProcess.length} chunks com metadados de posiÃ§Ã£o.`
    );

    console.log('ğŸ”¢ Gerando embeddings para cada chunk...');
    const chunksContent = documentsToProcess.map((doc) => doc.content);
    const { embeddings } = await embedMany({
      model: embeddingModel,
      values: chunksContent,
    });

    if (embeddings.length !== documentsToProcess.length) {
      throw new Error('A contagem de embeddings e chunks nÃ£o corresponde.');
    }

    console.log(`âœ… ${embeddings.length} embeddings gerados.`);

    console.log('ğŸ’¾ Salvando chunks e embeddings no banco de dados...');
    const documentsToInsert = documentsToProcess.map((doc, index) => ({
      fileId: fileRecord.id,
      content: doc.content,
      embedding: embeddings[index],
      metadata: doc.metadata,
    }));

    await db.insert(fileChunk).values(documentsToInsert);

    console.log('ğŸ‰ Processo finalizado com sucesso!');
  } catch (error) {
    console.error('âŒ Ocorreu um erro no script:', error);
  }
})();
